<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jialian Wu</title>

    <meta name="author" content="Jialian Wu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="shortcut icon" href="images/ub.ico" type="image/x-icon" />
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jialian Wu <span style="font-family: KaiTi, STKaiti, serif; font-size: 20px;">吴嘉濂</span>
                </p>
                <p>I'm a senior research scientist working on LLM at <a href="https://amdgenai.github.io/team/">AMD GenAI</a> in San Jose, CA. 
                  I received my Ph.D. degree in Computer Science and Engineering from State 
                  University of New York at Buffalo (UB) in 2023, advised by 
                  <a href="https://cse.buffalo.edu/~jsyuan/index.html">Prof. Junsong Yuan</a>. 
                  Before that, I got B.Eng. from Tianjin University in 2018. 
                  I received the Best Graduate Research Award and Best First Year Achiever
                   Award from the CSE department of UB.
                </p>
                <p style="text-align:center">
                  <a href="mailto:Jialian.Wu@amd.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Resume_Jialian_Wu.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=6Abc8OgAAAAJ&hl=en&oi=ao">Google scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/jialian-wu-0bb173162">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/JialianW">Github</a> &nbsp;/&nbsp;
                  <a href="https://x.com/Wu_Jialian" target="_blank">
                    <img src="images/x.svg" style="width:16px;height:16px; vertical-align: middle; position: relative; top: -2px;">
                  </a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/Jialian.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/Jialian.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          We're hiring interns for 2026 to work on LLM, agents, and RL. Time and location are flexible; feel free to reach out if interested.
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>
                Publications <a href="https://scholar.google.com/citations?user=6Abc8OgAAAAJ&hl=en&oi=ao" style="font-size:0.75em;  text-decoration:none;">(full list)</a>
              </h2>


              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://rocm.blogs.amd.com/artificial-intelligence/instella-long-context/README.html" id="MCG_journal">
                  <span class="papertitle">Instella-Long: A Fully Open Language Model with Long-Context Capability</span>
                </a>
                <br>
                <b>Jialian Wu</b>, Jiang Liu, Sudhanshu Ranjan, Xiaodong Yu, Gowtham Ramesh, Prakamya Mishra, Zicheng Liu, et al.
                <br>
                <em>tech report</em>, 2025
                <br>
                <a href="https://rocm.blogs.amd.com/artificial-intelligence/instella-long-context/README.html">Blog</a> /
                <a href="https://github.com/AMD-AIG-AIMA/Instella/tree/instella-long">Code</a> /
                <a href="https://huggingface.co/amd/Instella-3B-Long-Instruct">Huggingface</a>
              </td>
            </tr> 
            
            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://rocm.blogs.amd.com/artificial-intelligence/introducing-instella-3B/README.html" id="MCG_journal">
                  <span class="papertitle">Instella: New State-of-the-art Fully Open 3B Language Models</span>
                </a>
                <br>
                Jiang Liu, <b>Jialian Wu</b>, Xiaodong Yu, Prakamya Mishra, Sudhanshu Ranjan, Zicheng Liu, et al.
                <br>
                <em>tech report</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2511.10628">PDF</a> /
                <a href="https://github.com/AMD-AIG-AIMA/Instella">Code</a> /
                <a href="https://huggingface.co/collections/amd/instella-67c8a2c56e9198c85a97dd08">Huggingface</a>
              </td>
            </tr> 

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://www.libangzheng.com/lvr-project-page/" id="MCG_journal">
                  <span class="papertitle">Latent Visual Reasoning</span>
                </a>
                <br>
                Bangzheng Li, Ximeng Sun, Jiang Liu, Ze Wang, <b>Jialian Wu</b>, Xiaodong Yu, Hao Chen, Emad Barsoum, Muhao Chen, Zicheng Liu
                <br>
                <em>arXiv</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2509.24251">PDF</a> /
                <a href="https://github.com/VincentLeebang/lvr">Code</a>
              </td>
            </tr> 
            
            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://videomarathon.github.io/" id="MCG_journal">
                  <span class="papertitle">Unleashing Hour-Scale Video Training for Long Video-Language Understanding</span>
                </a>
                <br>
                Jingyang Lin, <b>Jialian Wu</b>, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Xiaodong Yu, Hao Chen, Jiebo Luo, Zicheng Liu, Emad Barsoum
                <br>
                <b><em>NeurIPS</em>, 2025 (Spotlight)</b>
                <br>
                <a href="https://arxiv.org/pdf/2506.05332">PDF</a> /
                <a href="https://github.com/jylins/hourllava">Code</a> /
                <a href="https://huggingface.co/datasets/jylins/videomarathon">Dataset</a>
              </td>
            </tr> 

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://agentlaboratory.github.io/" id="MCG_journal">
                  <span class="papertitle">Agent Laboratory: Using LLM Agents as Research Assistants</span>
                </a>
                <br>
                Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, <b>Jialian Wu</b>, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum
                <br>
                <b><em>EMNLP Findings</em>, 2025</b>
                <br>
                <a href="https://arxiv.org/pdf/2501.04227">PDF</a> /
                <a href="https://github.com/SamuelSchmidgall/AgentLaboratory">Code</a>
                <span style="display:inline-block; margin-left:6px;">
                  <a class="github-button" href="https://github.com/SamuelSchmidgall/AgentLaboratory"
                     data-color-scheme="no-preference: light; light: light_high_contrast; dark: dark;"
                     data-icon="octicon-star" data-size="small" data-show-count="true"
                     aria-label="Star on GitHub">Star</a>
                </span>
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://prakamya-mishra.github.io/TTTBench/" id="MCG_journal">
                  <span class="papertitle">TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games</span>
                </a>
                <br>
                Prakamya Mishra, Jiang Liu, <b>Jialian Wu</b>, Xiaodong Yu, Zicheng Liu, Emad Barsoum
                <br>
                <b><em>EMNLP</em>, 2025</b>
                <br>
                <a href="https://arxiv.org/pdf/2506.10209">PDF</a> /
                <a href="https://huggingface.co/datasets/amd/TTT-Bench">Dataset</a>
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2502.15920" id="MCG_journal">
                  <span class="papertitle">Self-Taught Agentic Long Context Understanding</span>
                </a>
                <br>
                Yufan Zhuang, Xiaodong Yu, <b>Jialian Wu</b>, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Jingbo Shang, Zicheng Liu, Emad Barsoum.
                <br>
                <b><em>ACL</em>, 2025</b>
                <br>
                <a href="https://arxiv.org/pdf/2502.15920">PDF</a> /
                <a href="https://github.com/EvanZhuang/AgenticLU">Code</a> 
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2212.00280" id="MCG_journal">
                  <span class="papertitle">GRiT: A Generative Region-to-text Transformer for Object Understanding</span>
                </a>
                <br>
                <b>Jialian Wu</b>, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, Lijuan Wang
                <br>
                <b><em>ECCV</em>, 2024</b>
                <br>
                <a href="https://arxiv.org/pdf/2212.00280">PDF</a> /
                <a href="https://github.com/JialianW/GRiT">Code</a>
                <!-- <span style="display:inline-block; margin-left:6px;">
                  <a class="github-button" href="https://github.com/JialianW/GRiT"
                     data-color-scheme="no-preference: light; light: light_high_contrast; dark: dark;"
                     data-icon="octicon-star" data-size="small" data-show-count="true"
                     aria-label="Star on GitHub">Star</a>
                </span> -->
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Efficient_Video_Instance_Segmentation_via_Tracklet_Query_and_Proposal_CVPR_2022_paper.pdf" id="MCG_journal">
                  <span class="papertitle">Efficient Video Instance Segmentation via Tracklet Query and Proposal</span>
                </a>
                <br>
                <b>Jialian Wu</b>, Sudhir Yarram, Hui Liang, Tian Lan, Junsong Yuan, Jayan Eledath, Gerard Medioni
                <br>
                <b><em>CVPR</em>, 2022</b>
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Efficient_Video_Instance_Segmentation_via_Tracklet_Query_and_Proposal_CVPR_2022_paper.pdf">PDF</a> /
                <a href="https://www.youtube.com/watch?v=sSPMzgtMKCE&feature=youtu.be">Video</a> 
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2103.08808" id="MCG_journal">
                  <span class="papertitle">Track to Detect and Segment: An Online Multi-Object Tracker</span>
                </a>
                <br>
                <b>Jialian Wu</b>, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, Junsong Yuan
                <br>
                <b><em>CVPR</em>, 2021</b>
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Track_To_Detect_and_Segment_An_Online_Multi-Object_Tracker_CVPR_2021_paper.pdf">PDF</a> /
                <a href="https://github.com/JialianW/TraDeS">Code</a> /
                <a href="https://www.youtube.com/watch?v=oGNtSFHRZJA">Video</a>
                <!-- <span style="display:inline-block; margin-left:6px;">
                  <a class="github-button" href="https://github.com/JialianW/TraDeS"
                     data-color-scheme="no-preference: light; light: light_high_contrast; dark: dark;"
                     data-icon="octicon-star" data-size="small" data-show-count="true"
                     aria-label="Star on GitHub">Star</a>
                </span> -->
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Song_Stacked_Homography_Transformations_for_Multi-View_Pedestrian_Detection_ICCV_2021_paper.pdf" id="MCG_journal">
                  <span class="papertitle">Stacked Homography Transformations for Multi-View Pedestrian Detection</span>
                </a>
                <br>
                Liangchen Song, <b>Jialian Wu</b>, Ming Yang, Qian Zhang, Yuan Li, Junsong Yuan
                <br>
                <b><em>ICCV</em>, 2021 (Oral)</b>
                <br>
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Song_Stacked_Homography_Transformations_for_Multi-View_Pedestrian_Detection_ICCV_2021_paper.pdf">PDF</a>
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://cse.buffalo.edu/~jsyuan/papers/2021/ACM_MM2021_liangchen-1.pdf" id="MCG_journal">
                  <span class="papertitle">Handling Difficult Labels for Multi-label Image Classification via Uncertainty Distillation</span>
                </a>
                <br>
                Liangchen Song, <b>Jialian Wu</b>, Ming Yang, Qian Zhang, Yuan Li, Junsong Yuan
                <br>
                <b><em>ACM MM</em>, 2021</b>
                <br>
                <a href="https://cse.buffalo.edu/~jsyuan/papers/2021/ACM_MM2021_liangchen-1.pdf">PDF</a>
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://cse.buffalo.edu/~jsyuan/papers/2021/aaai21_song.pdf" id="MCG_journal">
                  <span class="papertitle">Robust Knowledge Transfer via Hybrid Forward on the Teacher-Student Model</span>
                </a>
                <br>
                Liangchen Song, <b>Jialian Wu</b>, Ming Yang, Qian Zhang, Yuan Li, Junsong Yuan
                <br>
                <b><em>AAAI</em>, 2021</b>
                <br>
                <a href="https://cse.buffalo.edu/~jsyuan/papers/2021/aaai21_song.pdf">PDF</a>
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Temporal-Context_Enhanced_Detection_of_Heavily_Occluded_Pedestrians_CVPR_2020_paper.pdf" id="MCG_journal">
                  <span class="papertitle">Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians</span>
                </a>
                <br>
                <b>Jialian Wu</b>, Chunluan Zhou, Ming Yang, Qian Zhang, Yuan Li, Junsong Yuan
                <br>
                <b><em>CVPR</em>, 2020</b>
                <br>
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Temporal-Context_Enhanced_Detection_of_Heavily_Occluded_Pedestrians_CVPR_2020_paper.pdf">PDF</a>
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2008.05676" id="MCG_journal">
                  <span class="papertitle">Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance Segmentation</span>
                </a>
                <br>
                <b>Jialian Wu</b>, Liangchen Song, Tiancai Wang, Qian Zhang, Junsong Yuan
                <br>
                <b><em>ACM MM</em>, 2020 / <em>IEEE Transactions on Multimedia</em>, 2021</b>
                <br>
                <a href="https://arxiv.org/pdf/2008.05676v2">PDF</a> /
                <a href="https://github.com/JialianW/Forest_RCNN">Code</a>
              </td>
            </tr>

            <tr>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://cse.buffalo.edu/~jsyuan/papers/2020/SML.pdf" id="MCG_journal">
                  <span class="papertitle">Self-Mimic Learning for Small-scale Pedestrian Detection</span>
                </a>
                <br>
                <b>Jialian Wu</b>, Chunluan Zhou, Qian Zhang, Ming Yang, Junsong Yuan
                <br>
                <b><em>ACM MM</em>, 2020</b>
                <br>
                <a href="https://cse.buffalo.edu/~jsyuan/papers/2020/SML.pdf">PDF</a>
              </td>
            </tr>


    



    
	

  




            

            

          </tbody></table>


          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:2px;"><tbody>
            <tr>
              <td>
                <h2>Mentorship</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
          <tr>
            <td>
              <p>It's fortunate to collaborate with talented PhD students! Feel free to reach out if you’re interested in an internship at AMD.</p>
              <ul>
                <li><a href="https://kaijiezhu11.github.io/">Kaijie Zhu</a>, UCSB, 2025</li>
                <li><a href="https://jylin.me/">Jingyang Lin</a>, University of Rochester, 2024–2025</li>
              </ul>
            </td>
          </tr>

          
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:2px;"><tbody>
            <tr>
              <td>
                <h2>Internships</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/amazon-science-logo.svg" width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                Applied Scientist Intern, Amazon, Seattle, WA
                <br>
                August - October, 2022.
                Advisor: Dr. <a href="https://cs.stanford.edu/~taranlan/">Tian Lan</a> and Dr. <a href="https://sites.google.com/site/seraphlh/">Hui Liang</a>
                <br>
                Project: Video Instance Segmentation
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/Microsoft.png" width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                Research Intern, Redmond, WA
                <br>
                May-August, 2022. Advisor: Dr. <a href="https://scholar.google.com/citations?user=vJWEw_8AAAAJ&hl=en">Jianfeng Wang</a>, Dr. <a href="https://zhegan27.github.io/">Zhe Gan</a>, <br> Dr. <a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ&hl=en">Lijuan Wang</a>, Dr. <a href="https://zyang-ur.github.io//">Zhengyuan Yang</a>, Dr. <a href="https://scholar.google.com/citations?user=bkALdvsAAAAJ&hl=en">Zicheng Liu</a>
                <br>
                Project: Object Understanding with Natural Language
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/amazon-science-logo.svg" width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                Applied Scientist Intern, Amazon, Seattle, WA
                <br>
                May-August, 2021.
                Advisor: Dr. <a href="https://cs.stanford.edu/~taranlan/">Tian Lan</a> and Dr. <a href="https://sites.google.com/site/seraphlh/">Hui Liang</a>
                <br>
                Project: Video Instance Segmentation
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/horizon-robotics.png" width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                Research Intern, Cupertino, CA
                <br>
                May-August, 2020.
                Advisor: Dr. <a href="https://scholar.google.com/citations?user=uNj2w9gAAAAJ&hl=en">Yu Wang</a> and Dr. <a href="https://scholar.google.com/citations?user=uBHJx08AAAAJ&hl=en">Ming Yang</a>
                <br>
                Research Intern, Beijing.
                <br>
                05/2018 - 07/2019.
                Advisor: Dr. <a href="https://scholar.google.com/citations?user=pCY-bikAAAAJ&hl=zh-CN">Qian Zhang</a> and Dr. <a href="https://scholar.google.com/citations?user=uBHJx08AAAAJ&hl=en">Ming Yang</a>.
                <br>
                Projects: Object tracking and detection
              </td>
            </tr>   
            
          </tbody></table>

          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:2px;"><tbody>
            <tr>
              <td>
                <h2>Professional Services</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr>
              <ul>
                <li><b>Conference Reviewer:</b></li>
                CVPR, ICCV, ECCV, ICLR, ICML, NeurIPS, Outstanding reviewer in CVPR 2021
                <li><b>Journal Reviewer:</b></li>
                TPAMI, TIP, TMM, TCSVT
              </ul>  
            </tr>


            
            
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;">
                  <a href="music.html"><b>Music is a great passion in my life! (Listen in)</b></a>
                </p> 
                <p style="text-align:center;font-size:small;">
                  Template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
